{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba064bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python scripts/preprocess_urban8k.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e803ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python trainer.py --max_lr 3e-4 --run_name r1 --emb_dim 128 --dataset urban8k --seq_len 90112 --mix_ratio 1 --epoch_mix 12 --mix_loss bce --batch_size 128 --n_epochs 3500 --ds_factors 4 4 4 4 --amp --save_path outputs\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f226a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.helper_funcs import accuracy, mAP\n",
    "from datasets.batch_augs import BatchAugs\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a665c41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--dim_feedforward'], dest='dim_feedforward', nargs=None, const=None, default=512, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "'''train'''\n",
    "parser.add_argument(\"--max_lr\", default=3e-4, type=float)\n",
    "parser.add_argument(\"--wd\", default=1e-5, type=float)\n",
    "parser.add_argument(\"--batch_size\", default=128, type=int)\n",
    "parser.add_argument(\"--run_name\", default=None, type=Path)\n",
    "parser.add_argument('--loss_type', default=\"label_smooth\", type=str)\n",
    "parser.add_argument('--n_epochs', default=None, type=int)\n",
    "parser.add_argument('--epoch_mix', default=None, type=int)\n",
    "parser.add_argument(\"--amp\", action='store_true')\n",
    "parser.add_argument(\"--filter_bias_and_bn\", action='store_true', default=True)\n",
    "parser.add_argument(\"--ext_pretrained\", default=None, type=str)\n",
    "parser.add_argument(\"--multilabel\", action='store_true')\n",
    "parser.add_argument('--save_path', default=None, type=Path)\n",
    "parser.add_argument('--load_path', default=None, type=Path)\n",
    "parser.add_argument('--scheduler', default=None, type=str)\n",
    "parser.add_argument('--augs_signal', nargs='+', type=str,\n",
    "                    default=['amp', 'neg', 'tshift', 'tmask', 'ampsegment', 'cycshift'])\n",
    "parser.add_argument('--augs_noise', nargs='+', type=str,\n",
    "                    default=['awgn', 'abgn', 'apgn', 'argn', 'avgn', 'aun', 'phn', 'sine'])\n",
    "parser.add_argument('--augs_mix', nargs='+', type=str, default=['mixup', 'timemix', 'freqmix', 'phmix'])\n",
    "parser.add_argument('--mix_loss', default='bce', type=str)\n",
    "parser.add_argument('--mix_ratio', default=0.5, type=float)\n",
    "parser.add_argument('--ema', default=0.995, type=float)\n",
    "parser.add_argument('--log_interval', default=100, type=int)\n",
    "parser.add_argument(\"--kd_model\", default=None, type=Path)\n",
    "parser.add_argument(\"--use_bg\", action='store_true', default=False)\n",
    "parser.add_argument(\"--resume_training\", action='store_true', default=False)\n",
    "parser.add_argument(\"--use_balanced_sampler\", action='store_true', default=False)\n",
    "\n",
    "'''common'''\n",
    "parser.add_argument('--local_rank', default=0, type=int)\n",
    "parser.add_argument('--gpu_ids', nargs='+', default=[0])\n",
    "parser.add_argument(\"--use_ddp\", action='store_true')\n",
    "parser.add_argument(\"--use_dp\", action='store_true')\n",
    "parser.add_argument('--save_interval', default=100, type=int)\n",
    "parser.add_argument('--num_workers', default=8, type=int)\n",
    "\n",
    "'''data'''\n",
    "parser.add_argument('--fold_id', default=0, type=int)\n",
    "parser.add_argument(\"--data_subtype\", default='balanced', type=str)\n",
    "parser.add_argument('--seq_len', default=None, type=int)\n",
    "parser.add_argument('--dataset', default=None, type=str)\n",
    "'''net'''\n",
    "parser.add_argument('--ds_factors', nargs='+', type=int, default=[4, 4, 4, 4])\n",
    "parser.add_argument('--n_head', default=8, type=int)\n",
    "parser.add_argument('--n_layers', default=4, type=int)\n",
    "parser.add_argument(\"--emb_dim\", default=128, type=int)\n",
    "parser.add_argument(\"--model_type\", default='SoundNetRaw', type=str)\n",
    "parser.add_argument(\"--nf\", default=16, type=int)\n",
    "parser.add_argument(\"--dim_feedforward\", default=512, type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d02f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(['--max_lr', '3e-4', '--run_name', \"r1\", '--emb_dim', '128','--dataset', \"urban8k\",'--seq_len', '90112', '--mix_ratio', '1', '--epoch_mix', '12' ,'--mix_loss',\"bce\",'--batch_size', '32', '--n_epochs', '3500','--amp', '--save_path',\"outputs\" ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767742fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(max_lr=0.0003, wd=1e-05, batch_size=32, run_name=WindowsPath('r1'), loss_type='label_smooth', n_epochs=3500, epoch_mix=12, amp=True, filter_bias_and_bn=True, ext_pretrained=None, multilabel=False, save_path=WindowsPath('outputs'), load_path=None, scheduler=None, augs_signal=['amp', 'neg', 'tshift', 'tmask', 'ampsegment', 'cycshift'], augs_noise=['awgn', 'abgn', 'apgn', 'argn', 'avgn', 'aun', 'phn', 'sine'], augs_mix=['mixup', 'timemix', 'freqmix', 'phmix'], mix_loss='bce', mix_ratio=1.0, ema=0.995, log_interval=100, kd_model=None, use_bg=False, resume_training=False, use_balanced_sampler=False, local_rank=0, gpu_ids=[0], use_ddp=False, use_dp=False, save_interval=100, num_workers=8, fold_id=0, data_subtype='balanced', seq_len=90112, dataset='urban8k', ds_factors=[4, 4, 4, 4], n_head=8, n_layers=4, emb_dim=128, model_type='SoundNetRaw', nf=16, dim_feedforward=512)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3990147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_run(net, batch_sz, seq_len):\n",
    "    print(\"***********Dummy Run************\")\n",
    "    d = next(net.parameters()).device\n",
    "    x = torch.randn(batch_sz, 1, seq_len, device=d, requires_grad=False)\n",
    "    t_batch = time.time()\n",
    "    with torch.no_grad():\n",
    "        for k in range(10):\n",
    "            _ = net(x)\n",
    "    t_batch = (time.time()-t_batch)/10\n",
    "    print(\"dummy succededd, avg_time_batch:{}ms\".format(t_batch*1000))\n",
    "    del x\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_args(args):\n",
    "    if args.augs_noise[0] == 'none':\n",
    "        args.augs_noise = []\n",
    "    if args.augs_mix[0] == 'none':\n",
    "        args.augs_mix = []\n",
    "    return args\n",
    "\n",
    "\n",
    "def create_dataset(args):\n",
    "    ##################################################################################\n",
    "    # ESC-50\n",
    "    ##################################################################################\n",
    "    if args.dataset == 'esc50':\n",
    "        from datasets.esc_dataset import ESCDataset as SoundDataset\n",
    "        train_set = SoundDataset(\n",
    "            args.data_path,\n",
    "            mode='train',\n",
    "            segment_length=args.seq_len,\n",
    "            sampling_rate=args.sampling_rate,\n",
    "            transforms=args.augs_signal + args.augs_noise,\n",
    "            fold_id=args.fold_id\n",
    "        )\n",
    "\n",
    "        test_set = SoundDataset(\n",
    "            args.data_path,\n",
    "            mode='test',\n",
    "            segment_length=args.seq_len,\n",
    "            sampling_rate=args.sampling_rate,\n",
    "            transforms=None,\n",
    "            fold_id=args.fold_id\n",
    "        )\n",
    "\n",
    "    ##################################################################################\n",
    "    # SpeechCommands V2-35\n",
    "    ##################################################################################\n",
    "    elif args.dataset == 'speechcommands':\n",
    "        from datasets.speechcommand_dataset import SpeechCommandsDataset as SoundDataset\n",
    "        train_set = SoundDataset(\n",
    "            args.data_path,\n",
    "            mode='train',\n",
    "            segment_length=args.seq_len,\n",
    "            sampling_rate=args.sampling_rate,\n",
    "            transforms=args.augs_signal + args.augs_noise,\n",
    "            use_background=args.use_bg\n",
    "        )\n",
    "\n",
    "        test_set = SoundDataset(\n",
    "            args.data_path,\n",
    "            mode='val',\n",
    "            segment_length=args.seq_len,\n",
    "            sampling_rate=args.sampling_rate,\n",
    "            transforms=None,\n",
    "            use_background=False\n",
    "        )\n",
    "\n",
    "    ##################################################################################\n",
    "    # AudioSet\n",
    "    ##################################################################################\n",
    "    elif args.dataset == 'audioset':\n",
    "        from datasets.audioset_dataset import AudioSetDataset as SoundDataset\n",
    "\n",
    "        train_set = SoundDataset(\n",
    "            args.data_path,\n",
    "            'train',\n",
    "            data_subtype=args.data_subtype,\n",
    "            segment_length=args.seq_len,\n",
    "            sampling_rate=args.sampling_rate,\n",
    "            transforms=args.augs_signal + args.augs_noise,\n",
    "        )\n",
    "\n",
    "        test_set = SoundDataset(\n",
    "            args.data_path,\n",
    "            'test',\n",
    "            data_subtype=None,\n",
    "            segment_length=args.seq_len,\n",
    "            sampling_rate=args.sampling_rate,\n",
    "            transforms=None,\n",
    "        )\n",
    "\n",
    "    ##################################################################################\n",
    "    # Urban8K\n",
    "    ##################################################################################\n",
    "    elif args.dataset == 'urban8k':\n",
    "        from datasets.urban8K_dataset import Urban8KDataset as SoundDataset\n",
    "\n",
    "        train_set = SoundDataset(\n",
    "            args.data_path,\n",
    "            'train',\n",
    "            segment_length=args.seq_len,\n",
    "            sampling_rate=args.sampling_rate,\n",
    "            transforms=args.augs_signal + args.augs_noise,\n",
    "            fold_id=args.fold_id,\n",
    "        )\n",
    "\n",
    "        test_set = SoundDataset(\n",
    "            args.data_path,\n",
    "            'test',\n",
    "            segment_length=args.seq_len,\n",
    "            sampling_rate=args.sampling_rate,\n",
    "            transforms=None,\n",
    "            fold_id=args.fold_id\n",
    "        )\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "def create_model(args):\n",
    "    from modules.soundnet import SoundNetRaw as SoundNet\n",
    "    ds_fac = np.prod(np.array(args.ds_factors)) * 4\n",
    "    net = SoundNet(nf=args.nf,\n",
    "                   dim_feedforward=args.dim_feedforward,\n",
    "                   clip_length=args.seq_len // ds_fac,\n",
    "                   embed_dim=args.emb_dim,\n",
    "                   n_layers=args.n_layers,\n",
    "                   nhead=args.n_head,\n",
    "                   n_classes=args.n_classes,\n",
    "                   factors=args.ds_factors,\n",
    "                   )\n",
    "    return net\n",
    "\n",
    "\n",
    "def save_model(net, opt, loss, best_loss, acc, best_acc, steps, root, lr_scheduler=None, scaler=None):\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_loss = loss\n",
    "        chkpnt = {\n",
    "            'best_acc': best_acc,\n",
    "            'model_dict': net.state_dict(),\n",
    "            'opt_dict': opt.state_dict(),\n",
    "            'steps': steps,\n",
    "            'best_loss': best_loss,\n",
    "        }\n",
    "        if lr_scheduler is not None:\n",
    "            chkpnt['lr_scheduler'] = lr_scheduler.state_dict()\n",
    "        if scaler is not None:\n",
    "            chkpnt['scaler'] = scaler.state_dict()\n",
    "        torch.save(chkpnt, root / \"chkpnt.pt\")\n",
    "        torch.save(net.state_dict(), root / \"best_model.pt\")\n",
    "        print(best_acc, 'saved')\n",
    "\n",
    "    elif acc == best_acc:\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            chkpnt = {\n",
    "                'best_acc': best_acc,\n",
    "                'model_dict': net.state_dict(),\n",
    "                'opt_dict': opt.state_dict(),\n",
    "                'steps': steps,\n",
    "                'best_loss': best_loss,\n",
    "            }\n",
    "            if lr_scheduler is not None:\n",
    "                chkpnt['lr_scheduler'] = lr_scheduler.state_dict()\n",
    "            torch.save(chkpnt, root / \"chkpnt.pt\")\n",
    "            torch.save(net.state_dict(), root / \"best_model.pt\")\n",
    "            print(best_acc, 'saved')\n",
    "    return best_acc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4e58a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == 'esc50':\n",
    "    args.data_path = r'../data/ESC/ESC-50'\n",
    "    args.sampling_rate = 22050\n",
    "    args.n_classes = 50\n",
    "elif args.dataset == 'audioset':\n",
    "    args.data_path = r'../data/audioset'\n",
    "    args.sampling_rate = 22050\n",
    "    args.n_classes = 527\n",
    "elif args.dataset == 'speechcommands':\n",
    "    args.data_path = r'data/speech_commands_V2'\n",
    "    args.sampling_rate = 16000\n",
    "    args.n_classes = 35\n",
    "elif args.dataset == 'urban8k':\n",
    "    args.data_path = r'data/UrbanSound8K'\n",
    "    #args.data_path = r'http://localhost:8890/edit/AudioClassfication-main/data/UrbanSound8K/'\n",
    "    args.sampling_rate = 22050\n",
    "    args.n_classes = 10\n",
    "else:\n",
    "    raise ValueError(\"Wrong dataset in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b125c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'urban8k'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e420dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1770"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.urban8K_dataset import Urban8KDataset as SoundDataset\n",
    "test_set = SoundDataset(\n",
    "            args.data_path,\n",
    "            'test',\n",
    "            segment_length=args.seq_len,\n",
    "            sampling_rate=args.sampling_rate,\n",
    "            transforms=None,\n",
    "            fold_id=0\n",
    "        )\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff79e7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8732\n",
      "8732\n",
      "1770 6962\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# Create data loaders #\n",
    "#######################\n",
    "train_set, test_set = create_dataset(args)\n",
    "print(len(test_set),len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd4b4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if args.multilabel:\n",
    "    from utils.helper_funcs import collate_fn\n",
    "    if args.use_balanced_sampler:\n",
    "        sampler = torch.utils.data.sampler.WeightedRandomSampler(train_set.samples_weight, train_set.__len__(), replacement=True)\n",
    "        train_loader = DataLoader(train_set, batch_size=args.batch_size,\n",
    "                                  num_workers=args.num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  shuffle=False,\n",
    "                                  drop_last=True,\n",
    "                                  collate_fn=collate_fn,\n",
    "                                  sampler=sampler\n",
    "                                  )\n",
    "    else:\n",
    "        train_loader = DataLoader(train_set, batch_size=args.batch_size,\n",
    "                                  num_workers=args.num_workers,\n",
    "                                  pin_memory=True,\n",
    "                                  shuffle=True,\n",
    "                                  drop_last=True,\n",
    "                                  collate_fn=collate_fn,\n",
    "                                  )\n",
    "    test_loader = DataLoader(test_set, batch_size=args.batch_size,\n",
    "                             num_workers=args.num_workers,\n",
    "                             pin_memory=True,\n",
    "                             shuffle=False,\n",
    "                             collate_fn=collate_fn,\n",
    "                             )\n",
    "else:\n",
    "    train_loader = DataLoader(train_set,\n",
    "                              batch_size=args.batch_size,\n",
    "                              num_workers=args.num_workers,\n",
    "                              pin_memory=True,\n",
    "                              shuffle=False if train_set is None else True,\n",
    "                              drop_last=True,\n",
    "                              )\n",
    "    test_loader = DataLoader(test_set,\n",
    "                             batch_size=args.batch_size,\n",
    "                             num_workers=args.num_workers,\n",
    "                             pin_memory=True,\n",
    "                             shuffle=False,\n",
    "                             )\n",
    "\n",
    "#####################\n",
    "# Network           #\n",
    "#####################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "660001f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1770"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5969239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_params = {\n",
    "        'seq_len': args.seq_len,\n",
    "        'fs': args.sampling_rate,\n",
    "        'device': device,\n",
    "        'augs': args.augs_mix,\n",
    "        'mix_ratio': args.mix_ratio,\n",
    "        'batch_sz': args.local_rank,\n",
    "        'epoch_mix': args.epoch_mix,\n",
    "        'resample_factors': [0.8, 0.9, 1.1, 1.2],\n",
    "        'multilabel': True if args.multilabel else False,\n",
    "        'mix_loss': args.mix_loss\n",
    "}\n",
    "batch_augs = BatchAugs(ba_params)\n",
    "\n",
    "if args.amp:\n",
    "    from torch.cuda.amp import GradScaler\n",
    "    scaler = GradScaler(init_scale=2**10)\n",
    "    eps = 1e-4\n",
    "else:\n",
    "    scaler = None\n",
    "    eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "869a773e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoundNetRaw(\n",
       "  (start): Sequential(\n",
       "    (0): ReflectionPad1d((3, 3))\n",
       "    (1): Conv1d(1, 16, kernel_size=(7,), stride=(1,), bias=False)\n",
       "    (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (down): Sequential(\n",
       "    (0): Down(\n",
       "      (down): Sequential(\n",
       "        (0): ReflectionPad1d((2, 2))\n",
       "        (1): Conv1d(16, 32, kernel_size=(5,), stride=(1,), bias=False)\n",
       "        (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (4): AADownsample()\n",
       "      )\n",
       "    )\n",
       "    (1): ResBlock1dTF(\n",
       "      (block_t): Sequential(\n",
       "        (0): ReflectionPad1d((7, 7))\n",
       "        (1): Conv1d(32, 32, kernel_size=(15,), stride=(1,), groups=32, bias=False)\n",
       "        (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (block_f): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (shortcut): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (2): Down(\n",
       "      (down): Sequential(\n",
       "        (0): ReflectionPad1d((2, 2))\n",
       "        (1): Conv1d(32, 64, kernel_size=(5,), stride=(1,), bias=False)\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (4): AADownsample()\n",
       "      )\n",
       "    )\n",
       "    (3): Down(\n",
       "      (down): Sequential(\n",
       "        (0): ReflectionPad1d((2, 2))\n",
       "        (1): Conv1d(64, 128, kernel_size=(5,), stride=(1,), bias=False)\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (4): AADownsample()\n",
       "      )\n",
       "    )\n",
       "    (4): ResBlock1dTF(\n",
       "      (block_t): Sequential(\n",
       "        (0): ReflectionPad1d((7, 7))\n",
       "        (1): Conv1d(128, 128, kernel_size=(15,), stride=(1,), groups=128, bias=False)\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (block_f): Sequential(\n",
       "        (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (shortcut): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (5): Down(\n",
       "      (down): Sequential(\n",
       "        (0): ReflectionPad1d((2, 2))\n",
       "        (1): Conv1d(128, 256, kernel_size=(5,), stride=(1,), bias=False)\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (4): AADownsample()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): Sequential(\n",
       "    (0): ResBlock1dTF(\n",
       "      (block_t): Sequential(\n",
       "        (0): ReflectionPad1d((7, 7))\n",
       "        (1): Conv1d(256, 256, kernel_size=(15,), stride=(1,), groups=256, bias=False)\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (block_f): Sequential(\n",
       "        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (shortcut): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (1): ResBlock1dTF(\n",
       "      (block_t): Sequential(\n",
       "        (0): ReflectionPad1d((21, 21))\n",
       "        (1): Conv1d(256, 256, kernel_size=(15,), stride=(1,), dilation=(3,), groups=256, bias=False)\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (block_f): Sequential(\n",
       "        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (shortcut): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (2): ResBlock1dTF(\n",
       "      (block_t): Sequential(\n",
       "        (0): ReflectionPad1d((63, 63))\n",
       "        (1): Conv1d(256, 256, kernel_size=(15,), stride=(1,), dilation=(9,), groups=256, bias=False)\n",
       "        (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (block_f): Sequential(\n",
       "        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (shortcut): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (3): Down(\n",
       "      (down): Sequential(\n",
       "        (0): ReflectionPad1d((1, 1))\n",
       "        (1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), bias=False)\n",
       "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (4): AADownsample()\n",
       "      )\n",
       "    )\n",
       "    (4): ResBlock1dTF(\n",
       "      (block_t): Sequential(\n",
       "        (0): ReflectionPad1d((7, 7))\n",
       "        (1): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512, bias=False)\n",
       "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (block_f): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (shortcut): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (5): ResBlock1dTF(\n",
       "      (block_t): Sequential(\n",
       "        (0): ReflectionPad1d((21, 21))\n",
       "        (1): Conv1d(512, 512, kernel_size=(15,), stride=(1,), dilation=(3,), groups=512, bias=False)\n",
       "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (block_f): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (shortcut): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (6): ResBlock1dTF(\n",
       "      (block_t): Sequential(\n",
       "        (0): ReflectionPad1d((63, 63))\n",
       "        (1): Conv1d(512, 512, kernel_size=(15,), stride=(1,), dilation=(9,), groups=512, bias=False)\n",
       "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (block_f): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "      (shortcut): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (7): Down(\n",
       "      (down): Sequential(\n",
       "        (0): ReflectionPad1d((1, 1))\n",
       "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), bias=False)\n",
       "        (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (4): AADownsample()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (project): Conv1d(1024, 128, kernel_size=(1,), stride=(1,))\n",
       "  (tf): TAggregate(\n",
       "    (transformer_enc): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = create_model(args)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f1e0c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.ext_pretrained is not None:\n",
    "    pre = ''\n",
    "    print(\"loading model for pretraining \", (Path(pre + args.ext_pretrained) / Path(\"model.pt\")).is_file())\n",
    "    net_ext = torch.load(Path(pre + args.ext_pretrained) / Path(\"model.pt\"))\n",
    "    with (args.ext_pretrained / Path(\"args.yml\")).open() as f:\n",
    "        args_pretrained = yaml.load(f, Loader=yaml.Loader)\n",
    "    try:\n",
    "        args_pretrained = vars(args_pretrained)\n",
    "    except:\n",
    "        pass\n",
    "    from modules.soundnet import SoundNetRaw as SoundNet\n",
    "    ds_fac = np.prod(np.array(args_pretrained['ds_factors'])) * 4\n",
    "    net = SoundNet(\n",
    "        nf=args['nf'],\n",
    "        dim_feedforward=args['dim_feedforward'],\n",
    "        clip_length=args['seq_len'] // ds_fac,\n",
    "        embed_dim=args['emb_dim'],\n",
    "        n_layers=args['n_layers'],\n",
    "        nhead=args['n_head'],\n",
    "        n_classes=args['n_classes'],\n",
    "        factors=args['ds_factors'],\n",
    "         )\n",
    "    try:\n",
    "        net.load_state_dict(net_ext, strict=True)\n",
    "    except:\n",
    "        '''remove module. prefix in case of DataParallel module'''\n",
    "        from collections import OrderedDict\n",
    "        state_dict = OrderedDict()\n",
    "        for k, v in net_ext.items():\n",
    "            name = k.replace('module.', '')\n",
    "            state_dict[name] = v\n",
    "        else:\n",
    "            net.load_state_dict(state_dict, strict=True)\n",
    "    del net_ext\n",
    "    nn = args.seq_len // (np.prod(np.array(args.ds_factors)) * 4) + 1\n",
    "    net.tf.pos_embed.data = F.interpolate(net.tf.pos_embed.data.transpose(2, 1), size=nn).transpose(2, 1)\n",
    "    net.tf.fc = torch.nn.Linear(args.emb_dim, args.n_classes)\n",
    "    net.to(device)\n",
    "\n",
    "if args.kd_model:\n",
    "    print(\"Loading teacher model {}\".format(args.kd_model))\n",
    "    with (args.kd_model / Path(\"args.yml\")).open() as f:\n",
    "        args_t = yaml.load(f, Loader=yaml.Loader)\n",
    "    try:\n",
    "        args_t = vars(args_t)\n",
    "    except:\n",
    "        pass\n",
    "    from modules.soundnet import SoundNetRaw as SoundNet\n",
    "    net_t = SoundNet(\n",
    "        nf=args_t['nf'],\n",
    "        dim_feedforward=args_t['dim_feedforward'],\n",
    "        clip_length=args_t['seq_len'] // ds_fac,\n",
    "        embed_dim=args_t['emb_dim'],\n",
    "        n_layers=args_t['n_layers'],\n",
    "        nhead=args_t['n_head'],\n",
    "        n_classes=args_t['n_classes'],\n",
    "        factors=args_t['ds_factors']\n",
    "    )\n",
    "    if (args.kd_model / Path('model.pt')).is_file():\n",
    "        teacher = torch.load(args.kd_model / Path('model.pt'), map_location=torch.device(device))\n",
    "    else:\n",
    "        chkpnt = torch.load(args.kd_model / Path('chkpnt.pt'), map_location=torch.device(device))\n",
    "        teacher = chkpnt['model_dict']\n",
    "    try:\n",
    "        net_t.load_state_dict(teacher, strict=True)\n",
    "    except:\n",
    "        '''remove module. prefix in case of DataParallel module'''\n",
    "        from collections import OrderedDict\n",
    "        state_dict = OrderedDict()\n",
    "        for k, v in teacher.items():\n",
    "            name = k.replace('module.', '')\n",
    "            state_dict[name] = v\n",
    "        net_t.load_state_dict(state_dict, strict=True)\n",
    "    net_t.eval()\n",
    "    net_t.to(device)\n",
    "    del args_t, teacher\n",
    "\n",
    "if args.use_dp:\n",
    "    args.gpu_ids = [i for i in range(torch.cuda.device_count())]\n",
    "    net = torch.nn.DataParallel(net, device_ids=args.gpu_ids)\n",
    "    if args.kd_model:\n",
    "        net_t = torch.nn.parallel.DataParallel(net_t, device_ids=args.gpu_ids)\n",
    "    print(\"Using Data Parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ebe79a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(max_lr=0.0003, wd=1e-05, batch_size=32, run_name=WindowsPath('r1'), loss_type='label_smooth', n_epochs=3500, epoch_mix=12, amp=True, filter_bias_and_bn=True, ext_pretrained=None, multilabel=False, save_path='outputs', load_path=None, scheduler=None, augs_signal=['amp', 'neg', 'tshift', 'tmask', 'ampsegment', 'cycshift'], augs_noise=['awgn', 'abgn', 'apgn', 'argn', 'avgn', 'aun', 'phn', 'sine'], augs_mix=['mixup', 'timemix', 'freqmix', 'phmix'], mix_loss='bce', mix_ratio=1.0, ema=0.995, log_interval=100, kd_model=None, use_bg=False, resume_training=False, use_balanced_sampler=False, local_rank=0, gpu_ids=[0], use_ddp=False, use_dp=False, save_interval=100, num_workers=8, fold_id=0, data_subtype='balanced', seq_len=90112, dataset='urban8k', ds_factors=[4, 4, 4, 4], n_head=8, n_layers=4, emb_dim=128, model_type='SoundNetRaw', nf=16, dim_feedforward=512, data_path='data/UrbanSound8K', sampling_rate=22050, n_classes=10)\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# optimizer         #\n",
    "#####################\n",
    "if args.filter_bias_and_bn:\n",
    "    from utils.helper_funcs import add_weight_decay\n",
    "    parameters = add_weight_decay(net, args.wd)\n",
    "else:\n",
    "    parameters = net.parameters()\n",
    "\n",
    "opt = torch.optim.AdamW(parameters,\n",
    "                        lr=args.max_lr,\n",
    "                        betas=[0.9, 0.99],\n",
    "                        weight_decay=0,\n",
    "                        eps=eps)\n",
    "\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(opt,\n",
    "                                                   max_lr=args.max_lr,\n",
    "                                                   steps_per_epoch=len(train_loader),\n",
    "                                                   epochs=args.n_epochs,\n",
    "                                                   pct_start=0.1,\n",
    "                                                   )\n",
    "\n",
    "if args.ema:\n",
    "    from modules.ema import ModelEma as EMA\n",
    "    ema = EMA(net, decay_per_epoch=args.ema)\n",
    "    epochs_from_last_reset = 0\n",
    "    decay_per_epoch_orig = args.ema\n",
    "\n",
    "#####################\n",
    "# losses            #\n",
    "#####################\n",
    "if args.loss_type == \"label_smooth\":\n",
    "    from modules.losses import LabelSmoothCrossEntropyLoss\n",
    "    criterion = LabelSmoothCrossEntropyLoss(smoothing=0.1, reduction='sum').to(device)\n",
    "\n",
    "elif args.loss_type == \"cross_entropy\":\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='sum').to(device)\n",
    "\n",
    "elif args.loss_type == \"focal\":\n",
    "    from modules.losses import FocalLoss\n",
    "    criterion = FocalLoss().to(device)\n",
    "\n",
    "elif args.loss_type == 'bce':\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='sum').to(device)\n",
    "\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "####################################\n",
    "# Dump arguments and create logger #\n",
    "####################################\n",
    "args.save_path='outputs'\n",
    "root = args.save_path/args.run_name\n",
    "root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "load_root = Path(args.load_path) if args.load_path else None\n",
    "with open(root / \"args.yml\", \"w\") as f:\n",
    "    yaml.dump(args, f)\n",
    "print(args)\n",
    "writer = SummaryWriter(str(root))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c01abd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "best_acc = -1\n",
    "best_loss = 999\n",
    "steps = 0\n",
    "if load_root and load_root.exists():\n",
    "    chkpnt = torch.load(load_root / \"chkpnt.pt\")\n",
    "    try:\n",
    "        net.load_state_dict(chkpnt['model_dict'], strict=True)\n",
    "    except:\n",
    "        '''remove module. prefix in case of DataParallel module'''\n",
    "        from collections import OrderedDict\n",
    "        state_dict = OrderedDict()\n",
    "        for k, v in chkpnt['model_dict'].items():\n",
    "            name = k.replace('module.', '')\n",
    "            state_dict[name] = v\n",
    "        net.load_state_dict(state_dict, strict=True)\n",
    "        del state_dict\n",
    "    if args.resume_training:\n",
    "        opt.load_state_dict(chkpnt['opt_dict'])\n",
    "        if scaler is chkpnt.keys() and chkpnt['scaler'] is not None:\n",
    "            scaler.load_state_dict(chkpnt[\"scaler\"])\n",
    "        if lr_scheduler is chkpnt.keys() and chkpnt['lr_scheduler'] is not None:\n",
    "            lr_scheduler.load_state_dict(chkpnt['lr_scheduler'])\n",
    "        steps = chkpnt['steps'] if 'steps' in chkpnt.keys() else 0\n",
    "\n",
    "    best_acc = chkpnt['best_acc']\n",
    "    if 'best_loss' in chkpnt.keys():\n",
    "        best_loss = chkpnt['best_loss']\n",
    "\n",
    "    print('checkpoints loaded')\n",
    "else:\n",
    "    best_acc = -1\n",
    "    best_loss = 999\n",
    "    steps = 0\n",
    "print(best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a36b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Dummy Run************\n",
      "dummy succededd, avg_time_batch:193.1509017944336ms\n",
      "epoch 1/3500 | iters 0/217 | ms/batch 44.72 | acc/loss [1.5625000e+01 8.2474144e+01 1.2000000e-05]\n",
      "52.824858757062145 saved\n",
      "test: Epoch 1 | Iters 99 / 56 | ms/batch 619.07 | acc/best acc/loss 52.82 52.82 1.45 1.45\n",
      "Took 36.9615s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 1/3500 | iters 100/217 | ms/batch  2.76 | acc/loss [3.91562500e+01 6.01351114e+01 1.20004045e-05]\n",
      "61.355932203389834 saved\n",
      "test: Epoch 1 | Iters 199 / 56 | ms/batch 613.05 | acc/best acc/loss 61.36 61.36 1.26 1.26\n",
      "Took 36.7873s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 1/3500 | iters 200/217 | ms/batch  2.59 | acc/loss [5.1812500e+01 5.1591917e+01 1.2002856e-05]\n",
      "epoch 1/3500 time 1.64\n",
      "64.46327683615819 saved\n",
      "test: Epoch 2 | Iters 82 / 56 | ms/batch 923.26 | acc/best acc/loss 64.46 64.46 1.15 1.15\n",
      "Took 36.0610s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 2/3500 | iters 83/217 | ms/batch  2.78 | acc/loss [5.66875000e+01 4.82455986e+01 1.20077714e-05]\n",
      "65.02824858757062 saved\n",
      "test: Epoch 2 | Iters 182 / 56 | ms/batch 608.58 | acc/best acc/loss 65.03 65.03 1.10 1.10\n",
      "Took 36.4288s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 2/3500 | iters 183/217 | ms/batch  2.66 | acc/loss [5.85000000e+01 4.65428696e+01 1.20151505e-05]\n",
      "epoch 2/3500 time 1.58\n",
      "67.45762711864407 saved\n",
      "test: Epoch 3 | Iters 65 / 56 | ms/batch 971.71 | acc/best acc/loss 67.46 67.46 1.07 1.07\n",
      "Took 40.1420s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 3/3500 | iters 66/217 | ms/batch  2.78 | acc/loss [6.11562500e+01 4.53328818e+01 1.20249932e-05]\n",
      "69.71751412429379 saved\n",
      "test: Epoch 3 | Iters 165 / 56 | ms/batch 708.01 | acc/best acc/loss 69.72 69.72 0.98 0.98\n",
      "Took 43.3629s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 3/3500 | iters 166/217 | ms/batch  2.74 | acc/loss [6.35937500e+01 4.46118155e+01 1.20372994e-05]\n",
      "epoch 3/3500 time 1.73\n",
      "test: Epoch 4 | Iters 48 / 56 | ms/batch 957.35 | acc/best acc/loss 69.66 69.72 0.99 0.98\n",
      "Took 36.1944s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 4/3500 | iters 49/217 | ms/batch  2.64 | acc/loss [6.39687500e+01 4.33095321e+01 1.20520687e-05]\n",
      "72.31638418079096 saved\n",
      "test: Epoch 4 | Iters 148 / 56 | ms/batch 614.23 | acc/best acc/loss 72.32 72.32 0.92 0.92\n",
      "Took 36.9337s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 4/3500 | iters 149/217 | ms/batch  2.62 | acc/loss [6.56562500e+01 4.26139244e+01 1.20691357e-05]\n",
      "epoch 4/3500 time 1.61\n",
      "73.33333333333333 saved\n",
      "test: Epoch 5 | Iters 31 / 56 | ms/batch 944.53 | acc/best acc/loss 73.33 73.33 0.90 0.90\n",
      "Took 36.5863s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 5/3500 | iters 32/217 | ms/batch  2.66 | acc/loss [6.62500000e+01 4.16696887e+01 1.20887870e-05]\n",
      "74.18079096045197 saved\n",
      "test: Epoch 5 | Iters 131 / 56 | ms/batch 614.63 | acc/best acc/loss 74.18 74.18 0.87 0.87\n",
      "Took 36.9812s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 5/3500 | iters 132/217 | ms/batch  2.60 | acc/loss [6.85312500e+01 4.09702158e+01 1.21109197e-05]\n",
      "epoch 5/3500 time 1.61\n",
      "test: Epoch 6 | Iters 14 / 56 | ms/batch 936.43 | acc/best acc/loss 74.01 74.18 0.86 0.87\n",
      "Took 35.7843s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 6/3500 | iters 15/217 | ms/batch  2.63 | acc/loss [6.88750000e+01 4.00664684e+01 1.21355145e-05]\n",
      "test: Epoch 6 | Iters 114 / 56 | ms/batch 678.08 | acc/best acc/loss 74.07 74.18 0.85 0.87\n",
      "Took 43.1675s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 6/3500 | iters 115/217 | ms/batch  2.90 | acc/loss [6.96875000e+01 3.97168439e+01 1.21625708e-05]\n",
      "76.21468926553672 saved\n",
      "test: Epoch 6 | Iters 214 / 56 | ms/batch 675.09 | acc/best acc/loss 76.21 76.21 0.82 0.82\n",
      "Took 41.8210s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 6/3500 | iters 215/217 | ms/batch  2.69 | acc/loss [6.90937500e+01 3.99181420e+01 1.21920881e-05]\n",
      "epoch 6/3500 time 2.09\n",
      "test: Epoch 7 | Iters 97 / 56 | ms/batch 954.13 | acc/best acc/loss 75.82 76.21 0.79 0.82\n",
      "Took 37.2198s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 7/3500 | iters 98/217 | ms/batch  2.66 | acc/loss [7.02812500e+01 3.94767509e+01 1.22240661e-05]\n",
      "78.07909604519774 saved\n",
      "test: Epoch 7 | Iters 197 / 56 | ms/batch 618.40 | acc/best acc/loss 78.08 78.08 0.76 0.76\n",
      "Took 37.2232s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 7/3500 | iters 198/217 | ms/batch  2.87 | acc/loss [7.15937500e+01 3.79571685e+01 1.22585041e-05]\n",
      "epoch 7/3500 time 1.62\n",
      "test: Epoch 8 | Iters 80 / 56 | ms/batch 942.43 | acc/best acc/loss 77.57 78.08 0.76 0.76\n",
      "Took 36.6358s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 8/3500 | iters 81/217 | ms/batch  3.14 | acc/loss [7.30000000e+01 3.76173768e+01 1.22954015e-05]\n",
      "test: Epoch 8 | Iters 180 / 56 | ms/batch 624.87 | acc/best acc/loss 76.95 78.08 0.77 0.76\n",
      "Took 37.2698s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 8/3500 | iters 181/217 | ms/batch  2.70 | acc/loss [7.32812500e+01 3.76114279e+01 1.23347577e-05]\n",
      "epoch 8/3500 time 1.62\n",
      "78.75706214689265 saved\n",
      "test: Epoch 9 | Iters 63 / 56 | ms/batch 968.01 | acc/best acc/loss 78.76 78.76 0.71 0.71\n",
      "Took 37.5127s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 9/3500 | iters 64/217 | ms/batch  2.84 | acc/loss [7.40625000e+01 3.71521439e+01 1.23765721e-05]\n",
      "79.2090395480226 saved\n",
      "test: Epoch 9 | Iters 163 / 56 | ms/batch 662.43 | acc/best acc/loss 79.21 79.21 0.71 0.71\n",
      "Took 41.2224s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 9/3500 | iters 164/217 | ms/batch  2.78 | acc/loss [7.46875000e+01 3.63790896e+01 1.24208439e-05]\n",
      "epoch 9/3500 time 1.68\n",
      "test: Epoch 10 | Iters 46 / 56 | ms/batch 964.12 | acc/best acc/loss 79.10 79.21 0.69 0.71\n",
      "Took 37.6820s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 10/3500 | iters 47/217 | ms/batch  2.65 | acc/loss [7.45000000e+01 3.63209339e+01 1.24675724e-05]\n",
      "test: Epoch 10 | Iters 146 / 56 | ms/batch 639.56 | acc/best acc/loss 79.15 79.21 0.69 0.71\n",
      "Took 39.1354s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 10/3500 | iters 147/217 | ms/batch  2.71 | acc/loss [7.55625000e+01 3.58335236e+01 1.25167567e-05]\n",
      "epoch 10/3500 time 1.65\n",
      "80.96045197740114 saved\n",
      "test: Epoch 11 | Iters 29 / 56 | ms/batch 1149.44 | acc/best acc/loss 80.96 80.96 0.65 0.65\n",
      "Took 46.2419s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 11/3500 | iters 30/217 | ms/batch  4.42 | acc/loss [7.56250000e+01 3.53310959e+01 1.25683961e-05]\n",
      "test: Epoch 11 | Iters 129 / 56 | ms/batch 872.27 | acc/best acc/loss 79.89 80.96 0.67 0.65\n",
      "Took 44.6855s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 11/3500 | iters 130/217 | ms/batch  4.38 | acc/loss [7.61875000e+01 3.53846543e+01 1.26224896e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11/3500 time 2.23\n",
      "test: Epoch 12 | Iters 12 / 56 | ms/batch 1248.49 | acc/best acc/loss 80.56 80.96 0.65 0.65\n",
      "Took 45.8199s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 12/3500 | iters 13/217 | ms/batch  4.31 | acc/loss [7.56875000e+01 3.53493079e+01 1.26790364e-05]\n",
      "test: Epoch 12 | Iters 112 / 56 | ms/batch 877.48 | acc/best acc/loss 80.62 80.96 0.65 0.65\n",
      "Took 45.5075s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 12/3500 | iters 113/217 | ms/batch  4.35 | acc/loss [7.66562500e+01 3.45457047e+01 1.27380353e-05]\n",
      "81.01694915254237 saved\n",
      "test: Epoch 12 | Iters 212 / 56 | ms/batch 882.67 | acc/best acc/loss 81.02 81.02 0.66 0.66\n",
      "Took 45.7372s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 12/3500 | iters 213/217 | ms/batch  4.42 | acc/loss [7.56250000e+01 3.54191788e+01 1.27990009e-05]\n",
      "epoch 12/3500 time 2.67\n",
      "test: Epoch 13 | Iters 95 / 56 | ms/batch 1274.75 | acc/best acc/loss 77.80 81.02 0.96 0.66\n",
      "Took 45.7763s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 13/3500 | iters 96/217 | ms/batch  4.46 | acc/loss [5.66562500e+01 1.46577173e+02 1.28627349e-05]\n",
      "test: Epoch 13 | Iters 195 / 56 | ms/batch 899.32 | acc/best acc/loss 77.57 81.02 0.91 0.66\n",
      "Took 46.4434s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 13/3500 | iters 196/217 | ms/batch  4.36 | acc/loss [5.64062500e+01 1.19821734e+02 1.29290599e-05]\n",
      "epoch 13/3500 time 2.26\n",
      "test: Epoch 14 | Iters 78 / 56 | ms/batch 1241.80 | acc/best acc/loss 78.25 81.02 0.85 0.66\n",
      "Took 44.6564s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 14/3500 | iters 79/217 | ms/batch  4.42 | acc/loss [5.56875000e+01 1.16276839e+02 1.29978329e-05]\n",
      "test: Epoch 14 | Iters 178 / 56 | ms/batch 882.93 | acc/best acc/loss 77.34 81.02 0.86 0.66\n",
      "Took 45.1794s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 14/3500 | iters 179/217 | ms/batch  4.54 | acc/loss [5.26250000e+01 1.16843090e+02 1.30690527e-05]\n",
      "epoch 14/3500 time 2.21\n",
      "test: Epoch 15 | Iters 61 / 56 | ms/batch 1252.63 | acc/best acc/loss 79.10 81.02 0.81 0.66\n",
      "Took 45.1485s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 15/3500 | iters 62/217 | ms/batch  4.37 | acc/loss [5.80625000e+01 1.13036206e+02 1.31427180e-05]\n",
      "test: Epoch 15 | Iters 161 / 56 | ms/batch 877.34 | acc/best acc/loss 78.36 81.02 0.80 0.66\n",
      "Took 44.8277s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 15/3500 | iters 162/217 | ms/batch  4.41 | acc/loss [5.37187500e+01 1.13661094e+02 1.32188277e-05]\n",
      "epoch 15/3500 time 2.21\n",
      "test: Epoch 16 | Iters 44 / 56 | ms/batch 1269.27 | acc/best acc/loss 78.64 81.02 0.76 0.66\n",
      "Took 46.2309s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 16/3500 | iters 45/217 | ms/batch  4.40 | acc/loss [5.73125000e+01 1.12710157e+02 1.32973804e-05]\n",
      "test: Epoch 16 | Iters 144 / 56 | ms/batch 905.24 | acc/best acc/loss 80.00 81.02 0.76 0.66\n",
      "Took 47.1997s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 16/3500 | iters 145/217 | ms/batch  4.52 | acc/loss [5.60312500e+01 1.11004104e+02 1.33783748e-05]\n",
      "epoch 16/3500 time 2.26\n",
      "test: Epoch 17 | Iters 27 / 56 | ms/batch 1309.09 | acc/best acc/loss 80.00 81.02 0.74 0.66\n",
      "Took 49.3084s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 17/3500 | iters 28/217 | ms/batch  4.31 | acc/loss [5.64687500e+01 1.12090855e+02 1.34618094e-05]\n",
      "test: Epoch 17 | Iters 127 / 56 | ms/batch 923.59 | acc/best acc/loss 79.60 81.02 0.75 0.66\n",
      "Took 48.7696s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 17/3500 | iters 128/217 | ms/batch  4.38 | acc/loss [5.39687500e+01 1.11717170e+02 1.35476829e-05]\n",
      "epoch 17/3500 time 2.32\n",
      "test: Epoch 18 | Iters 10 / 56 | ms/batch 1294.84 | acc/best acc/loss 80.17 81.02 0.73 0.66\n",
      "Took 47.8247s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 18/3500 | iters 11/217 | ms/batch  4.39 | acc/loss [5.61562500e+01 1.09695603e+02 1.36359938e-05]\n",
      "test: Epoch 18 | Iters 110 / 56 | ms/batch 930.42 | acc/best acc/loss 79.10 81.02 0.74 0.66\n",
      "Took 49.8638s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 18/3500 | iters 111/217 | ms/batch  4.45 | acc/loss [5.61250000e+01 1.10062625e+02 1.37267406e-05]\n",
      "test: Epoch 18 | Iters 210 / 56 | ms/batch 961.15 | acc/best acc/loss 79.66 81.02 0.73 0.66\n",
      "Took 52.1995s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 18/3500 | iters 211/217 | ms/batch  4.36 | acc/loss [5.70625000e+01 1.08255089e+02 1.38199217e-05]\n",
      "epoch 18/3500 time 2.84\n",
      "test: Epoch 19 | Iters 93 / 56 | ms/batch 1359.82 | acc/best acc/loss 78.98 81.02 0.73 0.66\n",
      "Took 50.5575s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 19/3500 | iters 94/217 | ms/batch  4.31 | acc/loss [6.00312500e+01 1.07989021e+02 1.39155355e-05]\n",
      "test: Epoch 19 | Iters 193 / 56 | ms/batch 945.59 | acc/best acc/loss 79.89 81.02 0.70 0.66\n",
      "Took 50.8266s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 19/3500 | iters 194/217 | ms/batch  4.49 | acc/loss [5.75625000e+01 1.09662799e+02 1.40135804e-05]\n",
      "epoch 19/3500 time 2.39\n",
      "test: Epoch 20 | Iters 76 / 56 | ms/batch 1382.36 | acc/best acc/loss 80.11 81.02 0.71 0.66\n",
      "Took 51.8700s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 20/3500 | iters 77/217 | ms/batch  4.56 | acc/loss [5.73437500e+01 1.08275049e+02 1.41140547e-05]\n",
      "test: Epoch 20 | Iters 176 / 56 | ms/batch 1011.85 | acc/best acc/loss 81.02 81.02 0.70 0.66\n",
      "Took 57.4978s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 20/3500 | iters 177/217 | ms/batch  4.83 | acc/loss [5.76875000e+01 1.09150968e+02 1.42169568e-05]\n",
      "epoch 20/3500 time 2.48\n",
      "test: Epoch 21 | Iters 59 / 56 | ms/batch 1466.39 | acc/best acc/loss 80.68 81.02 0.69 0.66\n",
      "Took 56.9807s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 21/3500 | iters 60/217 | ms/batch  4.59 | acc/loss [5.97500000e+01 1.09247558e+02 1.43222847e-05]\n",
      "test: Epoch 21 | Iters 159 / 56 | ms/batch 1045.58 | acc/best acc/loss 80.56 81.02 0.69 0.66\n",
      "Took 60.5108s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 21/3500 | iters 160/217 | ms/batch  4.50 | acc/loss [5.77812500e+01 1.08965900e+02 1.44300368e-05]\n",
      "epoch 21/3500 time 2.60\n",
      "81.35593220338983 saved\n",
      "test: Epoch 22 | Iters 42 / 56 | ms/batch 1443.31 | acc/best acc/loss 81.36 81.36 0.65 0.65\n",
      "Took 50.3999s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 22/3500 | iters 43/217 | ms/batch  2.60 | acc/loss [5.92812500e+01 1.06908116e+02 1.45393515e-05]\n",
      "81.52542372881356 saved\n",
      "test: Epoch 22 | Iters 142 / 56 | ms/batch 596.92 | acc/best acc/loss 81.53 81.53 0.66 0.66\n",
      "Took 34.8672s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 22/3500 | iters 143/217 | ms/batch  2.64 | acc/loss [5.70937500e+01 1.06763356e+02 1.46516681e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22/3500 time 1.98\n",
      "test: Epoch 23 | Iters 25 / 56 | ms/batch 943.25 | acc/best acc/loss 80.90 81.53 0.67 0.66\n",
      "Took 36.7328s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 23/3500 | iters 26/217 | ms/batch  2.72 | acc/loss [5.53750000e+01 1.08811355e+02 1.47666572e-05]\n",
      "test: Epoch 23 | Iters 125 / 56 | ms/batch 631.40 | acc/best acc/loss 80.06 81.53 0.68 0.66\n",
      "Took 37.7361s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 23/3500 | iters 126/217 | ms/batch  2.86 | acc/loss [5.72812500e+01 1.07347108e+02 1.48840029e-05]\n",
      "epoch 23/3500 time 1.63\n",
      "test: Epoch 24 | Iters 8 / 56 | ms/batch 979.26 | acc/best acc/loss 80.34 81.53 0.66 0.66\n",
      "Took 37.0789s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 24/3500 | iters 9/217 | ms/batch  2.76 | acc/loss [5.91562500e+01 1.08212843e+02 1.50026728e-05]\n",
      "test: Epoch 24 | Iters 108 / 56 | ms/batch 628.92 | acc/best acc/loss 81.19 81.53 0.67 0.66\n",
      "Took 37.5234s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 24/3500 | iters 109/217 | ms/batch  2.81 | acc/loss [6.04062500e+01 1.06275641e+02 1.51248813e-05]\n",
      "test: Epoch 24 | Iters 208 / 56 | ms/batch 633.76 | acc/best acc/loss 80.62 81.53 0.67 0.66\n",
      "Took 37.6854s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 24/3500 | iters 209/217 | ms/batch  2.76 | acc/loss [5.95312500e+01 1.06811109e+02 1.52495002e-05]\n",
      "epoch 24/3500 time 2.03\n",
      "test: Epoch 25 | Iters 91 / 56 | ms/batch 965.96 | acc/best acc/loss 81.13 81.53 0.66 0.66\n",
      "Took 37.6481s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 25/3500 | iters 92/217 | ms/batch  2.76 | acc/loss [5.72187500e+01 1.06638955e+02 1.53765274e-05]\n",
      "81.9774011299435 saved\n",
      "test: Epoch 25 | Iters 191 / 56 | ms/batch 631.92 | acc/best acc/loss 81.98 81.98 0.64 0.64\n",
      "Took 37.1446s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 25/3500 | iters 192/217 | ms/batch  2.68 | acc/loss [5.91250000e+01 1.07159171e+02 1.55059606e-05]\n",
      "epoch 25/3500 time 1.65\n",
      "test: Epoch 26 | Iters 74 / 56 | ms/batch 954.47 | acc/best acc/loss 81.53 81.98 0.64 0.64\n",
      "Took 36.7622s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 26/3500 | iters 75/217 | ms/batch  2.70 | acc/loss [5.99062500e+01 1.03455078e+02 1.56377978e-05]\n",
      "test: Epoch 26 | Iters 174 / 56 | ms/batch 615.07 | acc/best acc/loss 81.02 81.98 0.65 0.64\n",
      "Took 35.7649s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 26/3500 | iters 175/217 | ms/batch  2.67 | acc/loss [5.67500000e+01 1.05049742e+02 1.57720366e-05]\n",
      "epoch 26/3500 time 1.62\n",
      "82.42937853107344 saved\n",
      "test: Epoch 27 | Iters 57 / 56 | ms/batch 916.66 | acc/best acc/loss 82.43 82.43 0.63 0.63\n",
      "Took 35.2343s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 27/3500 | iters 58/217 | ms/batch  2.69 | acc/loss [5.88750000e+01 1.05525939e+02 1.59086747e-05]\n",
      "test: Epoch 27 | Iters 157 / 56 | ms/batch 606.39 | acc/best acc/loss 80.34 82.43 0.65 0.63\n",
      "Took 35.2554s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 27/3500 | iters 158/217 | ms/batch  2.76 | acc/loss [5.80312500e+01 1.05927091e+02 1.60477098e-05]\n",
      "epoch 27/3500 time 1.57\n",
      "test: Epoch 28 | Iters 40 / 56 | ms/batch 911.45 | acc/best acc/loss 81.92 82.43 0.62 0.63\n",
      "Took 34.3001s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 28/3500 | iters 41/217 | ms/batch  2.74 | acc/loss [5.69375000e+01 1.05347009e+02 1.61891395e-05]\n",
      "test: Epoch 28 | Iters 140 / 56 | ms/batch 607.37 | acc/best acc/loss 80.00 82.43 0.67 0.63\n",
      "Took 35.2766s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 28/3500 | iters 141/217 | ms/batch  2.75 | acc/loss [5.89375000e+01 1.04461972e+02 1.63329614e-05]\n",
      "epoch 28/3500 time 1.57\n",
      "82.42937853107344 saved\n",
      "test: Epoch 29 | Iters 23 / 56 | ms/batch 907.73 | acc/best acc/loss 82.43 82.43 0.62 0.62\n",
      "Took 34.0056s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 29/3500 | iters 24/217 | ms/batch  2.64 | acc/loss [5.95625000e+01 1.04179262e+02 1.64791730e-05]\n",
      "84.0677966101695 saved\n",
      "test: Epoch 29 | Iters 123 / 56 | ms/batch 604.31 | acc/best acc/loss 84.07 84.07 0.60 0.60\n",
      "Took 35.0381s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 29/3500 | iters 124/217 | ms/batch  2.68 | acc/loss [5.89687500e+01 1.04448355e+02 1.66277718e-05]\n",
      "epoch 29/3500 time 1.56\n",
      "test: Epoch 30 | Iters 6 / 56 | ms/batch 912.23 | acc/best acc/loss 83.28 84.07 0.59 0.60\n",
      "Took 33.9002s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 30/3500 | iters 7/217 | ms/batch  2.72 | acc/loss [5.91562500e+01 1.05269959e+02 1.67787554e-05]\n",
      "test: Epoch 30 | Iters 106 / 56 | ms/batch 605.67 | acc/best acc/loss 81.30 84.07 0.63 0.60\n",
      "Took 35.1725s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 30/3500 | iters 107/217 | ms/batch  2.67 | acc/loss [6.14375000e+01 1.03624043e+02 1.69321210e-05]\n",
      "test: Epoch 30 | Iters 206 / 56 | ms/batch 624.39 | acc/best acc/loss 81.98 84.07 0.62 0.60\n",
      "Took 36.5886s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 30/3500 | iters 207/217 | ms/batch  2.79 | acc/loss [6.00000000e+01 1.04410874e+02 1.70878661e-05]\n",
      "epoch 30/3500 time 1.93\n",
      "test: Epoch 31 | Iters 89 / 56 | ms/batch 919.66 | acc/best acc/loss 82.37 84.07 0.61 0.60\n",
      "Took 35.1584s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 31/3500 | iters 90/217 | ms/batch  2.69 | acc/loss [6.15312500e+01 1.04272109e+02 1.72459881e-05]\n",
      "test: Epoch 31 | Iters 189 / 56 | ms/batch 608.01 | acc/best acc/loss 82.49 84.07 0.62 0.60\n",
      "Took 35.1038s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 31/3500 | iters 190/217 | ms/batch  2.80 | acc/loss [6.04375000e+01 1.03633745e+02 1.74064841e-05]\n",
      "epoch 31/3500 time 1.58\n",
      "test: Epoch 32 | Iters 72 / 56 | ms/batch 909.78 | acc/best acc/loss 81.53 84.07 0.62 0.60\n",
      "Took 34.6501s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 32/3500 | iters 73/217 | ms/batch  2.77 | acc/loss [5.96875000e+01 1.04447214e+02 1.75693515e-05]\n",
      "test: Epoch 32 | Iters 172 / 56 | ms/batch 609.55 | acc/best acc/loss 81.81 84.07 0.61 0.60\n",
      "Took 35.2323s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 32/3500 | iters 173/217 | ms/batch  2.70 | acc/loss [6.08437500e+01 1.03879049e+02 1.77345037e-05]\n",
      "epoch 32/3500 time 1.57\n",
      "test: Epoch 33 | Iters 55 / 56 | ms/batch 936.73 | acc/best acc/loss 82.77 84.07 0.61 0.60\n",
      "Took 37.3238s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 33/3500 | iters 56/217 | ms/batch  2.71 | acc/loss [6.01875000e+01 1.04288896e+02 1.79005015e-05]\n",
      "test: Epoch 33 | Iters 155 / 56 | ms/batch 607.52 | acc/best acc/loss 81.13 84.07 0.61 0.60\n",
      "Took 35.1296s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 33/3500 | iters 156/217 | ms/batch  2.74 | acc/loss [6.20000000e+01 1.01318417e+02 1.80704425e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33/3500 time 1.59\n",
      "test: Epoch 34 | Iters 38 / 56 | ms/batch 910.04 | acc/best acc/loss 82.94 84.07 0.60 0.60\n",
      "Took 34.2701s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 34/3500 | iters 39/217 | ms/batch  2.67 | acc/loss [6.16562500e+01 1.01461226e+02 1.82416652e-05]\n",
      "test: Epoch 34 | Iters 138 / 56 | ms/batch 605.93 | acc/best acc/loss 82.82 84.07 0.59 0.60\n",
      "Took 35.0175s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 34/3500 | iters 139/217 | ms/batch  2.87 | acc/loss [5.97187500e+01 1.01140182e+02 1.84156433e-05]\n",
      "epoch 34/3500 time 1.57\n",
      "test: Epoch 35 | Iters 21 / 56 | ms/batch 911.80 | acc/best acc/loss 82.26 84.07 0.58 0.60\n",
      "Took 34.1624s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 35/3500 | iters 22/217 | ms/batch  2.64 | acc/loss [6.07812500e+01 1.02777557e+02 1.85926319e-05]\n",
      "84.0677966101695 saved\n",
      "test: Epoch 35 | Iters 121 / 56 | ms/batch 605.74 | acc/best acc/loss 84.07 84.07 0.56 0.56\n",
      "Took 35.2049s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 35/3500 | iters 122/217 | ms/batch  2.77 | acc/loss [5.80000000e+01 1.02121241e+02 1.87719716e-05]\n",
      "epoch 35/3500 time 1.57\n",
      "test: Epoch 36 | Iters 4 / 56 | ms/batch 914.02 | acc/best acc/loss 82.54 84.07 0.59 0.56\n",
      "Took 33.9815s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 36/3500 | iters 5/217 | ms/batch  2.76 | acc/loss [6.10312500e+01 1.03551182e+02 1.89536592e-05]\n",
      "test: Epoch 36 | Iters 104 / 56 | ms/batch 606.91 | acc/best acc/loss 82.82 84.07 0.60 0.56\n",
      "Took 35.3181s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 36/3500 | iters 105/217 | ms/batch  2.75 | acc/loss [6.21250000e+01 1.03292108e+02 1.91376918e-05]\n",
      "test: Epoch 36 | Iters 204 / 56 | ms/batch 612.34 | acc/best acc/loss 83.16 84.07 0.58 0.56\n",
      "Took 35.3342s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 36/3500 | iters 205/217 | ms/batch  2.66 | acc/loss [6.02500000e+01 1.01337855e+02 1.93240661e-05]\n",
      "epoch 36/3500 time 1.92\n",
      "test: Epoch 37 | Iters 87 / 56 | ms/batch 904.18 | acc/best acc/loss 83.11 84.07 0.58 0.56\n",
      "Took 34.6411s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 37/3500 | iters 88/217 | ms/batch  2.92 | acc/loss [5.96562500e+01 1.01984159e+02 1.95127789e-05]\n",
      "test: Epoch 37 | Iters 187 / 56 | ms/batch 609.78 | acc/best acc/loss 82.09 84.07 0.61 0.56\n",
      "Took 35.0530s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 37/3500 | iters 188/217 | ms/batch  2.82 | acc/loss [6.14062500e+01 1.04175545e+02 1.97038271e-05]\n",
      "epoch 37/3500 time 1.56\n",
      "test: Epoch 38 | Iters 70 / 56 | ms/batch 906.74 | acc/best acc/loss 82.26 84.07 0.57 0.56\n",
      "Took 34.7350s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 38/3500 | iters 71/217 | ms/batch  2.81 | acc/loss [5.93750000e+01 1.01413942e+02 1.98972073e-05]\n",
      "test: Epoch 38 | Iters 170 / 56 | ms/batch 608.87 | acc/best acc/loss 83.90 84.07 0.57 0.56\n",
      "Took 35.3396s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 38/3500 | iters 171/217 | ms/batch  2.84 | acc/loss [5.83750000e+01 1.01575979e+02 2.00929163e-05]\n",
      "epoch 38/3500 time 1.57\n",
      "test: Epoch 39 | Iters 53 / 56 | ms/batch 907.98 | acc/best acc/loss 83.45 84.07 0.57 0.56\n",
      "Took 34.3605s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 39/3500 | iters 54/217 | ms/batch  2.72 | acc/loss [6.22187500e+01 1.01693923e+02 2.02909506e-05]\n",
      "test: Epoch 39 | Iters 153 / 56 | ms/batch 603.63 | acc/best acc/loss 82.32 84.07 0.60 0.56\n",
      "Took 34.9468s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 39/3500 | iters 154/217 | ms/batch  2.68 | acc/loss [6.23750000e+01 1.00135357e+02 2.04913070e-05]\n",
      "epoch 39/3500 time 1.56\n",
      "test: Epoch 40 | Iters 36 / 56 | ms/batch 910.82 | acc/best acc/loss 83.16 84.07 0.57 0.56\n",
      "Took 34.4809s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 40/3500 | iters 37/217 | ms/batch  2.69 | acc/loss [6.24687500e+01 1.01268221e+02 2.06939819e-05]\n",
      "test: Epoch 40 | Iters 136 / 56 | ms/batch 609.12 | acc/best acc/loss 82.09 84.07 0.59 0.56\n",
      "Took 35.3855s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 40/3500 | iters 137/217 | ms/batch  2.77 | acc/loss [6.11562500e+01 1.01738602e+02 2.08989720e-05]\n",
      "epoch 40/3500 time 1.57\n",
      "test: Epoch 41 | Iters 19 / 56 | ms/batch 912.12 | acc/best acc/loss 83.56 84.07 0.57 0.56\n",
      "Took 34.0976s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 41/3500 | iters 20/217 | ms/batch  2.71 | acc/loss [6.14375000e+01 1.00854252e+02 2.11062737e-05]\n",
      "test: Epoch 41 | Iters 119 / 56 | ms/batch 613.90 | acc/best acc/loss 82.60 84.07 0.60 0.56\n",
      "Took 35.7975s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 41/3500 | iters 120/217 | ms/batch  2.73 | acc/loss [6.16250000e+01 1.00178447e+02 2.13158834e-05]\n",
      "epoch 41/3500 time 1.58\n",
      "test: Epoch 42 | Iters 2 / 56 | ms/batch 919.92 | acc/best acc/loss 83.33 84.07 0.56 0.56\n",
      "Took 33.8602s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 42/3500 | iters 3/217 | ms/batch  2.60 | acc/loss [6.32500000e+01 1.00215137e+02 2.15277976e-05]\n",
      "test: Epoch 42 | Iters 102 / 56 | ms/batch 608.77 | acc/best acc/loss 83.73 84.07 0.55 0.56\n",
      "Took 35.5662s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 42/3500 | iters 103/217 | ms/batch  2.82 | acc/loss [6.11250000e+01 1.00474771e+02 2.17420126e-05]\n",
      "85.42372881355932 saved\n",
      "test: Epoch 42 | Iters 202 / 56 | ms/batch 645.09 | acc/best acc/loss 85.42 85.42 0.52 0.52\n",
      "Took 38.2157s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 42/3500 | iters 203/217 | ms/batch  2.91 | acc/loss [6.09062500e+01 1.01569989e+02 2.19585249e-05]\n",
      "epoch 42/3500 time 1.97\n",
      "test: Epoch 43 | Iters 85 / 56 | ms/batch 950.67 | acc/best acc/loss 82.66 85.42 0.59 0.52\n",
      "Took 35.7614s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 43/3500 | iters 86/217 | ms/batch  2.94 | acc/loss [6.02500000e+01 1.00796326e+02 2.21759642e-05]\n",
      "test: Epoch 43 | Iters 185 / 56 | ms/batch 615.64 | acc/best acc/loss 84.12 85.42 0.54 0.52\n",
      "Took 35.8418s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 43/3500 | iters 186/217 | ms/batch  2.83 | acc/loss [6.03750000e+01 9.92610840e+01 2.23950232e-05]\n",
      "epoch 43/3500 time 1.61\n",
      "test: Epoch 44 | Iters 68 / 56 | ms/batch 912.71 | acc/best acc/loss 84.80 85.42 0.55 0.52\n",
      "Took 34.8737s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 44/3500 | iters 69/217 | ms/batch  2.72 | acc/loss [6.14687500e+01 9.93286594e+01 2.26173174e-05]\n",
      "test: Epoch 44 | Iters 168 / 56 | ms/batch 611.62 | acc/best acc/loss 84.24 85.42 0.54 0.52\n",
      "Took 35.4678s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 44/3500 | iters 169/217 | ms/batch  2.73 | acc/loss [6.32187500e+01 1.00405233e+02 2.28429353e-05]\n",
      "epoch 44/3500 time 1.57\n",
      "test: Epoch 45 | Iters 51 / 56 | ms/batch 912.02 | acc/best acc/loss 83.67 85.42 0.55 0.52\n",
      "Took 34.2847s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45/3500 | iters 52/217 | ms/batch  2.71 | acc/loss [6.63437500e+01 9.75674916e+01 2.30708315e-05]\n",
      "test: Epoch 45 | Iters 151 / 56 | ms/batch 608.61 | acc/best acc/loss 83.11 85.42 0.56 0.52\n",
      "Took 35.3423s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 45/3500 | iters 152/217 | ms/batch  2.80 | acc/loss [5.93437500e+01 1.01844755e+02 2.33010022e-05]\n",
      "epoch 45/3500 time 1.57\n",
      "test: Epoch 46 | Iters 34 / 56 | ms/batch 913.29 | acc/best acc/loss 84.75 85.42 0.53 0.52\n",
      "Took 34.5503s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 46/3500 | iters 35/217 | ms/batch  2.80 | acc/loss [6.32812500e+01 1.01021341e+02 2.35334434e-05]\n",
      "85.53672316384181 saved\n",
      "test: Epoch 46 | Iters 134 / 56 | ms/batch 606.98 | acc/best acc/loss 85.54 85.54 0.51 0.51\n",
      "Took 35.2228s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 46/3500 | iters 135/217 | ms/batch  2.84 | acc/loss [6.40625000e+01 1.00502020e+02 2.37681511e-05]\n",
      "epoch 46/3500 time 1.57\n",
      "test: Epoch 47 | Iters 17 / 56 | ms/batch 906.76 | acc/best acc/loss 83.50 85.54 0.54 0.51\n",
      "Took 33.7881s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 47/3500 | iters 18/217 | ms/batch  2.67 | acc/loss [6.29375000e+01 9.73174612e+01 2.40051214e-05]\n",
      "test: Epoch 47 | Iters 117 / 56 | ms/batch 603.99 | acc/best acc/loss 84.12 85.54 0.53 0.51\n",
      "Took 35.0631s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 47/3500 | iters 118/217 | ms/batch  2.74 | acc/loss [6.54062500e+01 9.76276988e+01 2.42443501e-05]\n",
      "epoch 47/3500 time 1.56\n",
      "test: Epoch 48 | Iters 0 / 56 | ms/batch 931.59 | acc/best acc/loss 85.37 85.54 0.50 0.51\n",
      "Took 34.7634s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 48/3500 | iters 1/217 | ms/batch  2.73 | acc/loss [6.23437500e+01 1.00343861e+02 2.44858331e-05]\n",
      "test: Epoch 48 | Iters 100 / 56 | ms/batch 604.45 | acc/best acc/loss 84.86 85.54 0.50 0.51\n",
      "Took 35.0944s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 48/3500 | iters 101/217 | ms/batch  2.97 | acc/loss [6.26875000e+01 1.00192210e+02 2.47295664e-05]\n",
      "85.70621468926554 saved\n",
      "test: Epoch 48 | Iters 200 / 56 | ms/batch 769.71 | acc/best acc/loss 85.71 85.71 0.53 0.53\n",
      "Took 50.1629s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 48/3500 | iters 201/217 | ms/batch  4.20 | acc/loss [6.33750000e+01 1.00201543e+02 2.49755458e-05]\n",
      "epoch 48/3500 time 2.10\n",
      "test: Epoch 49 | Iters 83 / 56 | ms/batch 1117.30 | acc/best acc/loss 84.29 85.71 0.54 0.53\n",
      "Took 37.1477s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 49/3500 | iters 84/217 | ms/batch  2.78 | acc/loss [6.22812500e+01 9.90933961e+01 2.52237670e-05]\n",
      "test: Epoch 49 | Iters 183 / 56 | ms/batch 635.60 | acc/best acc/loss 85.14 85.71 0.52 0.53\n",
      "Took 37.5796s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 49/3500 | iters 184/217 | ms/batch  2.81 | acc/loss [6.26250000e+01 1.01002565e+02 2.54742258e-05]\n",
      "epoch 49/3500 time 1.80\n",
      "test: Epoch 50 | Iters 66 / 56 | ms/batch 959.56 | acc/best acc/loss 85.20 85.71 0.49 0.53\n",
      "Took 37.5737s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 50/3500 | iters 67/217 | ms/batch  2.74 | acc/loss [6.29687500e+01 9.68887823e+01 2.57269180e-05]\n",
      "test: Epoch 50 | Iters 166 / 56 | ms/batch 626.01 | acc/best acc/loss 85.20 85.71 0.51 0.53\n",
      "Took 36.7925s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 50/3500 | iters 167/217 | ms/batch  3.04 | acc/loss [6.16562500e+01 9.75087286e+01 2.59818391e-05]\n",
      "epoch 50/3500 time 1.64\n",
      "test: Epoch 51 | Iters 49 / 56 | ms/batch 950.07 | acc/best acc/loss 84.80 85.71 0.52 0.53\n",
      "Took 36.4201s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 51/3500 | iters 50/217 | ms/batch  2.74 | acc/loss [6.16250000e+01 9.82134687e+01 2.62389849e-05]\n",
      "test: Epoch 51 | Iters 149 / 56 | ms/batch 629.85 | acc/best acc/loss 85.31 85.71 0.50 0.53\n",
      "Took 37.1868s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 51/3500 | iters 150/217 | ms/batch  2.75 | acc/loss [6.23125000e+01 9.91963564e+01 2.64983509e-05]\n",
      "epoch 51/3500 time 1.63\n",
      "test: Epoch 52 | Iters 32 / 56 | ms/batch 956.05 | acc/best acc/loss 83.90 85.71 0.54 0.53\n",
      "Took 36.4177s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 52/3500 | iters 33/217 | ms/batch  2.70 | acc/loss [6.42500000e+01 9.69433768e+01 2.67599327e-05]\n",
      "test: Epoch 52 | Iters 132 / 56 | ms/batch 633.94 | acc/best acc/loss 84.58 85.71 0.52 0.53\n",
      "Took 37.2600s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 52/3500 | iters 133/217 | ms/batch  2.70 | acc/loss [6.49062500e+01 9.75775095e+01 2.70237258e-05]\n",
      "epoch 52/3500 time 1.64\n",
      "test: Epoch 53 | Iters 15 / 56 | ms/batch 955.98 | acc/best acc/loss 85.20 85.71 0.51 0.53\n",
      "Took 36.0434s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 53/3500 | iters 16/217 | ms/batch  2.79 | acc/loss [6.29375000e+01 9.60749229e+01 2.72869961e-05]\n",
      "test: Epoch 53 | Iters 115 / 56 | ms/batch 619.95 | acc/best acc/loss 84.86 85.71 0.50 0.53\n",
      "Took 36.2263s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 53/3500 | iters 116/217 | ms/batch  2.72 | acc/loss [6.36875000e+01 9.72200677e+01 2.75511665e-05]\n",
      "test: Epoch 53 | Iters 215 / 56 | ms/batch 637.62 | acc/best acc/loss 84.58 85.71 0.51 0.53\n",
      "Took 37.5781s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 53/3500 | iters 216/217 | ms/batch  2.64 | acc/loss [6.46875000e+01 9.82932818e+01 2.78201839e-05]\n",
      "epoch 53/3500 time 2.01\n",
      "85.70621468926554 saved\n",
      "test: Epoch 54 | Iters 98 / 56 | ms/batch 1006.87 | acc/best acc/loss 85.71 85.71 0.48 0.48\n",
      "Took 39.3127s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 54/3500 | iters 99/217 | ms/batch  2.73 | acc/loss [6.30937500e+01 9.65836745e+01 2.80927111e-05]\n",
      "test: Epoch 54 | Iters 198 / 56 | ms/batch 666.60 | acc/best acc/loss 84.18 85.71 0.51 0.48\n",
      "Took 39.8766s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 54/3500 | iters 199/217 | ms/batch  2.89 | acc/loss [6.11562500e+01 9.84561266e+01 2.83674268e-05]\n",
      "epoch 54/3500 time 1.72\n",
      "test: Epoch 55 | Iters 81 / 56 | ms/batch 1094.89 | acc/best acc/loss 84.07 85.71 0.51 0.48\n",
      "Took 43.2184s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 55/3500 | iters 82/217 | ms/batch  2.69 | acc/loss [6.46875000e+01 9.67155431e+01 2.86443264e-05]\n",
      "86.15819209039547 saved\n",
      "test: Epoch 55 | Iters 181 / 56 | ms/batch 692.69 | acc/best acc/loss 86.16 86.16 0.49 0.49\n",
      "Took 42.6990s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 55/3500 | iters 182/217 | ms/batch  2.79 | acc/loss [6.31250000e+01 9.65112882e+01 2.89234050e-05]\n",
      "epoch 55/3500 time 1.83\n",
      "test: Epoch 56 | Iters 64 / 56 | ms/batch 990.71 | acc/best acc/loss 86.10 86.16 0.49 0.49\n",
      "Took 39.0629s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56/3500 | iters 65/217 | ms/batch  2.71 | acc/loss [6.55625000e+01 9.59631319e+01 2.92046579e-05]\n",
      "test: Epoch 56 | Iters 164 / 56 | ms/batch 657.18 | acc/best acc/loss 84.24 86.16 0.52 0.49\n",
      "Took 39.2003s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 56/3500 | iters 165/217 | ms/batch  2.80 | acc/loss [6.54062500e+01 9.25529244e+01 2.94880803e-05]\n",
      "epoch 56/3500 time 1.70\n",
      "test: Epoch 57 | Iters 47 / 56 | ms/batch 986.08 | acc/best acc/loss 84.80 86.16 0.51 0.49\n",
      "Took 36.3959s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 57/3500 | iters 48/217 | ms/batch  2.82 | acc/loss [6.39687500e+01 9.89022691e+01 2.97736673e-05]\n",
      "86.32768361581921 saved\n",
      "test: Epoch 57 | Iters 147 / 56 | ms/batch 681.52 | acc/best acc/loss 86.33 86.33 0.46 0.46\n",
      "Took 42.0818s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 57/3500 | iters 148/217 | ms/batch  2.75 | acc/loss [6.18125000e+01 9.57871621e+01 3.00614141e-05]\n",
      "epoch 57/3500 time 1.72\n",
      "test: Epoch 58 | Iters 30 / 56 | ms/batch 1036.31 | acc/best acc/loss 85.71 86.33 0.48 0.46\n",
      "Took 38.6647s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 58/3500 | iters 31/217 | ms/batch  2.74 | acc/loss [6.41562500e+01 9.61779899e+01 3.03513158e-05]\n",
      "test: Epoch 58 | Iters 130 / 56 | ms/batch 703.08 | acc/best acc/loss 84.75 86.33 0.47 0.46\n",
      "Took 43.2798s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 58/3500 | iters 131/217 | ms/batch  2.82 | acc/loss [6.48437500e+01 9.37595150e+01 3.06433673e-05]\n",
      "epoch 58/3500 time 1.79\n",
      "test: Epoch 59 | Iters 13 / 56 | ms/batch 1098.43 | acc/best acc/loss 85.54 86.33 0.46 0.46\n",
      "Took 43.6518s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 59/3500 | iters 14/217 | ms/batch  3.06 | acc/loss [6.49062500e+01 9.51101957e+01 3.09375637e-05]\n",
      "test: Epoch 59 | Iters 113 / 56 | ms/batch 646.33 | acc/best acc/loss 83.95 86.33 0.50 0.46\n",
      "Took 38.2259s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 59/3500 | iters 114/217 | ms/batch  2.74 | acc/loss [6.48125000e+01 9.44869724e+01 3.12338999e-05]\n",
      "test: Epoch 59 | Iters 213 / 56 | ms/batch 661.37 | acc/best acc/loss 83.90 86.33 0.50 0.46\n",
      "Took 39.8677s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 59/3500 | iters 214/217 | ms/batch  3.16 | acc/loss [6.27812500e+01 9.37657877e+01 3.15323709e-05]\n",
      "epoch 59/3500 time 2.19\n",
      "test: Epoch 60 | Iters 96 / 56 | ms/batch 944.25 | acc/best acc/loss 86.05 86.33 0.45 0.46\n",
      "Took 37.0109s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 60/3500 | iters 97/217 | ms/batch  2.73 | acc/loss [6.56562500e+01 9.32386377e+01 3.18329716e-05]\n",
      "86.61016949152543 saved\n",
      "test: Epoch 60 | Iters 196 / 56 | ms/batch 637.41 | acc/best acc/loss 86.61 86.61 0.44 0.44\n",
      "Took 37.7303s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 60/3500 | iters 197/217 | ms/batch  2.86 | acc/loss [6.37812500e+01 9.60288996e+01 3.21356968e-05]\n",
      "epoch 60/3500 time 1.63\n",
      "test: Epoch 61 | Iters 79 / 56 | ms/batch 958.16 | acc/best acc/loss 85.88 86.61 0.48 0.44\n",
      "Took 37.3328s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 61/3500 | iters 80/217 | ms/batch  2.85 | acc/loss [6.22187500e+01 9.70554789e+01 3.24405413e-05]\n",
      "86.89265536723164 saved\n",
      "test: Epoch 61 | Iters 179 / 56 | ms/batch 639.79 | acc/best acc/loss 86.89 86.89 0.46 0.46\n",
      "Took 37.9435s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 61/3500 | iters 180/217 | ms/batch  2.83 | acc/loss [6.40312500e+01 9.60184801e+01 3.27475000e-05]\n",
      "epoch 61/3500 time 1.65\n",
      "test: Epoch 62 | Iters 62 / 56 | ms/batch 953.82 | acc/best acc/loss 86.38 86.89 0.45 0.46\n",
      "Took 36.8044s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 62/3500 | iters 63/217 | ms/batch  2.75 | acc/loss [6.65937500e+01 9.31425531e+01 3.30565676e-05]\n",
      "test: Epoch 62 | Iters 162 / 56 | ms/batch 636.35 | acc/best acc/loss 82.26 86.89 0.57 0.46\n",
      "Took 37.6256s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 62/3500 | iters 163/217 | ms/batch  2.86 | acc/loss [6.48437500e+01 9.54514610e+01 3.33661438e-05]\n",
      "epoch 62/3500 time 1.64\n",
      "87.2316384180791 saved\n",
      "test: Epoch 63 | Iters 45 / 56 | ms/batch 981.54 | acc/best acc/loss 87.23 87.23 0.45 0.45\n",
      "Took 37.8673s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 63/3500 | iters 46/217 | ms/batch  2.87 | acc/loss [6.41250000e+01 9.66546848e+01 3.36778650e-05]\n",
      "test: Epoch 63 | Iters 145 / 56 | ms/batch 635.61 | acc/best acc/loss 85.71 87.23 0.47 0.45\n",
      "Took 37.7404s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 63/3500 | iters 146/217 | ms/batch  2.81 | acc/loss [6.57500000e+01 9.48284620e+01 3.39924769e-05]\n",
      "epoch 63/3500 time 1.67\n",
      "test: Epoch 64 | Iters 28 / 56 | ms/batch 980.13 | acc/best acc/loss 86.67 87.23 0.45 0.45\n",
      "Took 37.1331s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 64/3500 | iters 29/217 | ms/batch  2.80 | acc/loss [6.43437500e+01 9.58845930e+01 3.43074507e-05]\n",
      "test: Epoch 64 | Iters 128 / 56 | ms/batch 629.47 | acc/best acc/loss 86.61 87.23 0.44 0.45\n",
      "Took 37.1152s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 64/3500 | iters 129/217 | ms/batch  2.82 | acc/loss [6.51875000e+01 9.71717039e+01 3.46269410e-05]\n",
      "epoch 64/3500 time 1.66\n",
      "test: Epoch 65 | Iters 11 / 56 | ms/batch 958.57 | acc/best acc/loss 85.99 87.23 0.46 0.45\n",
      "Took 36.2374s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 65/3500 | iters 12/217 | ms/batch  2.85 | acc/loss [6.58750000e+01 9.61437096e+01 3.49485080e-05]\n",
      "test: Epoch 65 | Iters 111 / 56 | ms/batch 627.23 | acc/best acc/loss 86.27 87.23 0.46 0.45\n",
      "Took 37.0223s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 65/3500 | iters 112/217 | ms/batch  2.90 | acc/loss [6.27500000e+01 9.55210175e+01 3.52721462e-05]\n",
      "87.45762711864407 saved\n",
      "test: Epoch 65 | Iters 211 / 56 | ms/batch 647.58 | acc/best acc/loss 87.46 87.46 0.44 0.44\n",
      "Took 38.7196s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 65/3500 | iters 212/217 | ms/batch  2.71 | acc/loss [6.47500000e+01 9.48163646e+01 3.55978501e-05]\n",
      "epoch 65/3500 time 2.02\n",
      "test: Epoch 66 | Iters 94 / 56 | ms/batch 942.89 | acc/best acc/loss 85.59 87.46 0.46 0.44\n",
      "Took 36.9412s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 66/3500 | iters 95/217 | ms/batch  2.79 | acc/loss [6.57812500e+01 9.26870344e+01 3.59256141e-05]\n",
      "test: Epoch 66 | Iters 194 / 56 | ms/batch 635.05 | acc/best acc/loss 84.92 87.46 0.51 0.44\n",
      "Took 37.5023s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 66/3500 | iters 195/217 | ms/batch  2.79 | acc/loss [6.14687500e+01 9.84347284e+01 3.62554326e-05]\n",
      "epoch 66/3500 time 1.63\n",
      "test: Epoch 67 | Iters 77 / 56 | ms/batch 958.75 | acc/best acc/loss 86.44 87.46 0.47 0.44\n",
      "Took 38.1263s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67/3500 | iters 78/217 | ms/batch  2.78 | acc/loss [6.41562500e+01 9.27051352e+01 3.65873000e-05]\n",
      "test: Epoch 67 | Iters 177 / 56 | ms/batch 647.90 | acc/best acc/loss 86.84 87.46 0.45 0.44\n",
      "Took 38.6725s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 67/3500 | iters 178/217 | ms/batch  2.76 | acc/loss [6.64375000e+01 9.28130205e+01 3.69212105e-05]\n",
      "epoch 67/3500 time 1.66\n",
      "test: Epoch 68 | Iters 60 / 56 | ms/batch 986.78 | acc/best acc/loss 85.08 87.46 0.49 0.44\n",
      "Took 38.3241s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 68/3500 | iters 61/217 | ms/batch  2.82 | acc/loss [6.45625000e+01 9.65801151e+01 3.72571585e-05]\n",
      "test: Epoch 68 | Iters 160 / 56 | ms/batch 647.91 | acc/best acc/loss 87.01 87.46 0.43 0.44\n",
      "Took 38.7166s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 68/3500 | iters 161/217 | ms/batch  2.77 | acc/loss [6.53125000e+01 9.43541151e+01 3.75951382e-05]\n",
      "epoch 68/3500 time 1.68\n",
      "87.51412429378531 saved\n",
      "test: Epoch 69 | Iters 43 / 56 | ms/batch 961.37 | acc/best acc/loss 87.51 87.51 0.41 0.41\n",
      "Took 36.6074s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 69/3500 | iters 44/217 | ms/batch  2.82 | acc/loss [6.45937500e+01 9.24817651e+01 3.79351438e-05]\n",
      "test: Epoch 69 | Iters 143 / 56 | ms/batch 632.47 | acc/best acc/loss 86.50 87.51 0.45 0.41\n",
      "Took 37.2702s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 69/3500 | iters 144/217 | ms/batch  2.85 | acc/loss [6.63750000e+01 9.40513121e+01 3.82771695e-05]\n",
      "epoch 69/3500 time 1.65\n",
      "test: Epoch 70 | Iters 26 / 56 | ms/batch 964.27 | acc/best acc/loss 85.88 87.51 0.47 0.41\n",
      "Took 36.7321s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 70/3500 | iters 27/217 | ms/batch  2.83 | acc/loss [6.56250000e+01 9.31761877e+01 3.86212095e-05]\n",
      "test: Epoch 70 | Iters 126 / 56 | ms/batch 654.69 | acc/best acc/loss 87.40 87.51 0.44 0.41\n",
      "Took 39.6284s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 70/3500 | iters 127/217 | ms/batch  2.81 | acc/loss [6.52812500e+01 9.58173158e+01 3.89672579e-05]\n",
      "epoch 70/3500 time 1.67\n",
      "test: Epoch 71 | Iters 9 / 56 | ms/batch 930.19 | acc/best acc/loss 87.40 87.51 0.44 0.41\n",
      "Took 34.6511s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 71/3500 | iters 10/217 | ms/batch  2.67 | acc/loss [6.24375000e+01 9.68766647e+01 3.93153087e-05]\n",
      "test: Epoch 71 | Iters 109 / 56 | ms/batch 621.07 | acc/best acc/loss 85.93 87.51 0.45 0.41\n",
      "Took 36.7161s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 71/3500 | iters 110/217 | ms/batch  2.91 | acc/loss [6.55625000e+01 9.25522016e+01 3.96653561e-05]\n",
      "test: Epoch 71 | Iters 209 / 56 | ms/batch 621.36 | acc/best acc/loss 86.44 87.51 0.47 0.41\n",
      "Took 36.4811s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 71/3500 | iters 210/217 | ms/batch  2.78 | acc/loss [6.51875000e+01 9.39580227e+01 4.00173939e-05]\n",
      "epoch 71/3500 time 1.96\n",
      "test: Epoch 72 | Iters 92 / 56 | ms/batch 931.55 | acc/best acc/loss 85.31 87.51 0.49 0.41\n",
      "Took 35.7641s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 72/3500 | iters 93/217 | ms/batch  2.75 | acc/loss [6.45312500e+01 9.12116885e+01 4.03714162e-05]\n",
      "test: Epoch 72 | Iters 192 / 56 | ms/batch 609.82 | acc/best acc/loss 87.34 87.51 0.43 0.41\n",
      "Took 35.5014s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 72/3500 | iters 193/217 | ms/batch  2.80 | acc/loss [6.47500000e+01 9.29789084e+01 4.07265942e-05]\n",
      "epoch 72/3500 time 1.59\n",
      "test: Epoch 73 | Iters 75 / 56 | ms/batch 905.54 | acc/best acc/loss 86.38 87.51 0.44 0.41\n",
      "Took 34.4774s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 73/3500 | iters 76/217 | ms/batch  2.68 | acc/loss [6.69062500e+01 9.08346834e+01 4.10818006e-05]\n",
      "test: Epoch 73 | Iters 175 / 56 | ms/batch 614.81 | acc/best acc/loss 84.86 87.51 0.49 0.41\n",
      "Took 36.2100s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 73/3500 | iters 176/217 | ms/batch  2.79 | acc/loss [6.33437500e+01 9.29973962e+01 4.14417202e-05]\n",
      "epoch 73/3500 time 1.57\n",
      "88.19209039548022 saved\n",
      "test: Epoch 74 | Iters 58 / 56 | ms/batch 912.96 | acc/best acc/loss 88.19 88.19 0.43 0.43\n",
      "Took 34.7936s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 74/3500 | iters 59/217 | ms/batch  2.83 | acc/loss [6.75000000e+01 9.04322089e+01 4.18036000e-05]\n",
      "test: Epoch 74 | Iters 158 / 56 | ms/batch 608.00 | acc/best acc/loss 87.85 88.19 0.41 0.43\n",
      "Took 35.2190s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 74/3500 | iters 159/217 | ms/batch  2.71 | acc/loss [6.45312500e+01 9.28498210e+01 4.21674337e-05]\n",
      "epoch 74/3500 time 1.57\n",
      "test: Epoch 75 | Iters 41 / 56 | ms/batch 911.67 | acc/best acc/loss 87.12 88.19 0.45 0.43\n",
      "Took 34.2993s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 75/3500 | iters 42/217 | ms/batch  2.70 | acc/loss [6.47187500e+01 8.82445337e+01 4.25332151e-05]\n",
      "test: Epoch 75 | Iters 141 / 56 | ms/batch 607.67 | acc/best acc/loss 86.55 88.19 0.45 0.43\n",
      "Took 35.4495s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 75/3500 | iters 142/217 | ms/batch  2.96 | acc/loss [6.51250000e+01 9.01790927e+01 4.29009379e-05]\n",
      "epoch 75/3500 time 1.57\n",
      "test: Epoch 76 | Iters 24 / 56 | ms/batch 909.78 | acc/best acc/loss 87.01 88.19 0.43 0.43\n",
      "Took 34.1985s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 76/3500 | iters 25/217 | ms/batch  2.74 | acc/loss [6.43750000e+01 9.53442574e+01 4.32705959e-05]\n",
      "test: Epoch 76 | Iters 124 / 56 | ms/batch 601.72 | acc/best acc/loss 87.85 88.19 0.40 0.43\n",
      "Took 34.8928s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 76/3500 | iters 125/217 | ms/batch  2.79 | acc/loss [6.55625000e+01 9.16325638e+01 4.36421827e-05]\n",
      "epoch 76/3500 time 1.56\n",
      "test: Epoch 77 | Iters 7 / 56 | ms/batch 912.28 | acc/best acc/loss 86.16 88.19 0.46 0.43\n",
      "Took 34.0347s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 77/3500 | iters 8/217 | ms/batch  2.72 | acc/loss [6.78437500e+01 9.15418677e+01 4.40156920e-05]\n",
      "test: Epoch 77 | Iters 107 / 56 | ms/batch 603.93 | acc/best acc/loss 86.84 88.19 0.43 0.43\n",
      "Took 35.3447s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 77/3500 | iters 108/217 | ms/batch  2.74 | acc/loss [6.58750000e+01 9.02511058e+01 4.43911174e-05]\n",
      "test: Epoch 77 | Iters 207 / 56 | ms/batch 738.54 | acc/best acc/loss 86.61 88.19 0.45 0.43\n",
      "Took 48.4712s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 77/3500 | iters 208/217 | ms/batch  4.58 | acc/loss [6.55937500e+01 9.42643836e+01 4.47684523e-05]\n",
      "epoch 77/3500 time 2.07\n",
      "test: Epoch 78 | Iters 90 / 56 | ms/batch 1286.04 | acc/best acc/loss 86.38 88.19 0.42 0.43\n",
      "Took 46.8157s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 78/3500 | iters 91/217 | ms/batch  4.37 | acc/loss [6.65625000e+01 9.24755557e+01 4.51476905e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.47457627118644 saved\n",
      "test: Epoch 78 | Iters 190 / 56 | ms/batch 902.88 | acc/best acc/loss 88.47 88.47 0.40 0.40\n",
      "Took 46.8680s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 78/3500 | iters 191/217 | ms/batch  4.43 | acc/loss [6.46250000e+01 9.44375468e+01 4.55288254e-05]\n",
      "epoch 78/3500 time 2.27\n",
      "test: Epoch 79 | Iters 73 / 56 | ms/batch 1273.22 | acc/best acc/loss 87.80 88.47 0.41 0.40\n",
      "Took 46.8386s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 79/3500 | iters 74/217 | ms/batch  4.39 | acc/loss [6.60625000e+01 8.94394950e+01 4.59118505e-05]\n",
      "test: Epoch 79 | Iters 173 / 56 | ms/batch 894.20 | acc/best acc/loss 87.29 88.47 0.45 0.40\n",
      "Took 45.9692s to save samples\n",
      "----------------------------------------------------------------------------------------------------\n",
      "epoch 79/3500 | iters 174/217 | ms/batch  4.57 | acc/loss [6.68750000e+01 9.03688356e+01 4.62967592e-05]\n",
      "epoch 79/3500 time 2.25\n"
     ]
    }
   ],
   "source": [
    "# enable cudnn autotuner to speed up training\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "dummy_run(net, args.batch_size, args.seq_len)\n",
    "costs = []\n",
    "net.train()\n",
    "start = time.time()\n",
    "skip_scheduler = False\n",
    "for epoch in range(1, args.n_epochs + 1):\n",
    "    if args.use_ddp:\n",
    "        sampler.set_epoch(epoch)\n",
    "    t_epoch = time.time()\n",
    "\n",
    "    if epochs_from_last_reset <= 1:  # two first epochs do ultra short-term ema\n",
    "        ema.decay_per_epoch = 0.01\n",
    "    else:\n",
    "        ema.decay_per_epoch = decay_per_epoch_orig\n",
    "    epochs_from_last_reset += 1\n",
    "\n",
    "    # set 'decay_per_step' for the eooch\n",
    "    ema.set_decay_per_step(len(train_loader))\n",
    "\n",
    "    for iterno, (x, y) in enumerate(train_loader):\n",
    "        t_batch = time.time()\n",
    "        x = x.to(device)\n",
    "        if args.multilabel:\n",
    "            y = [F.one_hot(torch.Tensor(y_i).long(), args.n_classes).sum(dim=0).float() for y_i in y]\n",
    "            y = torch.stack(y, dim=0).contiguous().to(device)\n",
    "        else:\n",
    "            y = y.to(device)\n",
    "        x, targets, is_mixed = batch_augs(x, y, epoch)\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            pred = net(x)\n",
    "            if is_mixed:\n",
    "                loss_cls = batch_augs.mix_loss(pred, targets, n_classes=args.n_classes,\n",
    "                                               pred_one_hot=args.multilabel)\n",
    "            else:\n",
    "                loss_cls = criterion(pred, y)\n",
    "\n",
    "        if args.kd_model:\n",
    "            with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "                with torch.no_grad():\n",
    "                    pred_t = net_t(x)\n",
    "                if args.multilabel:\n",
    "                    loss_cls += F.kl_div(F.logsigmoid(pred), torch.sigmoid(pred_t), reduction='batchmean')\n",
    "                else:\n",
    "                    loss_cls += F.kl_div(pred.log_softmax(-1), pred_t.softmax(-1), reduction='batchmean')\n",
    "        ###################\n",
    "        # Train Generator #\n",
    "        ###################\n",
    "        net.zero_grad()\n",
    "        if args.amp:\n",
    "            scaler.scale(loss_cls).backward()\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "            scaler.step(opt)\n",
    "            amp_scale = scaler.get_scale()\n",
    "            scaler.update()\n",
    "            skip_scheduler = amp_scale != scaler.get_scale()\n",
    "        else:\n",
    "            loss_cls.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if args.ema:\n",
    "            ema.update(net, steps)\n",
    "\n",
    "\n",
    "        if not skip_scheduler:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        if not args.multilabel:\n",
    "            acc = accuracy(pred.detach().data, y.detach().data, topk=(1,))[0]\n",
    "            acc = acc.item()\n",
    "        else:\n",
    "            acc = mAP(y.detach().cpu().numpy(), torch.sigmoid(pred).detach().cpu().numpy())\n",
    "        costs.append([acc, loss_cls.item(), opt.param_groups[0]['lr']])\n",
    "        ######################\n",
    "        # Update tensorboard #\n",
    "        ######################\n",
    "        if steps % args.log_interval == 0:\n",
    "            if not args.use_ddp or (args.use_ddp and torch.distributed.get_rank() == 0):\n",
    "                writer.add_scalar(\"train/acc\", costs[-1][0], steps)\n",
    "                writer.add_scalar(\"train/ce\", costs[-1][1], steps)\n",
    "                writer.add_scalar(\"train/lr\", costs[-1][2], steps)\n",
    "\n",
    "            t_batch = time.time() - t_batch\n",
    "            print(\"epoch {}/{} | iters {}/{} | ms/batch {:5.2f} | acc/loss {}\".format(\n",
    "                epoch,\n",
    "                args.n_epochs,\n",
    "                iterno,\n",
    "                len(train_loader),\n",
    "                1000 * t_batch / args.log_interval,\n",
    "                np.asarray(costs).mean(0))\n",
    "            )\n",
    "            costs = []\n",
    "            start = time.time()\n",
    "\n",
    "        steps += 1\n",
    "        if steps % args.save_interval == 0:\n",
    "            ''' validate'''\n",
    "            net.eval()\n",
    "            st = time.time()\n",
    "            loss = 0\n",
    "            if args.multilabel:\n",
    "                labels = np.zeros((len(test_loader.dataset), args.n_classes)).astype(np.float32)\n",
    "                preds = np.zeros((len(test_loader.dataset), args.n_classes)).astype(np.float32)\n",
    "            else:\n",
    "                cm = np.zeros((args.n_classes, args.n_classes), dtype=np.int32)\n",
    "            idx_start = 0\n",
    "            with torch.no_grad():\n",
    "                acc = 0\n",
    "                for i, (x, y) in enumerate(test_loader):\n",
    "                    x = x.to(device)\n",
    "                    if args.multilabel:\n",
    "                        y = [F.one_hot(torch.Tensor(y_i).long(), args.n_classes).sum(dim=0).float() for y_i in y]\n",
    "                        y = torch.stack(y, dim=0).contiguous().to(device)\n",
    "                        y = y.to(device)\n",
    "                        pred = net(x)\n",
    "                        loss += F.binary_cross_entropy_with_logits(pred, y)\n",
    "                        idx_end = idx_start + y.shape[0]\n",
    "                        preds[idx_start:idx_end, :] = torch.sigmoid(pred).detach().data.cpu().numpy()\n",
    "                        labels[idx_start:idx_end, :] = y.detach().data.cpu().numpy()\n",
    "                        idx_start = idx_end\n",
    "                    else:\n",
    "                        y = y.to(device)\n",
    "                        pred = net(x)\n",
    "                        _, y_est = torch.max(pred, 1)\n",
    "                        loss += F.cross_entropy(pred, y)\n",
    "                        acc += accuracy(pred.detach().data, y.detach().data, topk=[1, ])[0].item()\n",
    "                        for t, p in zip(y.view(-1), y_est.view(-1)):\n",
    "                            cm[t.long(), p.long()] += 1\n",
    "                loss /= len(test_loader)\n",
    "\n",
    "                if args.multilabel:\n",
    "                    acc = mAP(labels, preds)\n",
    "                else:\n",
    "                    # acc /= len(test_loader)\n",
    "                    acc = 100*np.diag(cm).sum()/ len(test_loader.dataset)\n",
    "\n",
    "            writer.add_scalar(\"test/acc\", acc, steps)\n",
    "            writer.add_scalar(\"test/ce\", loss.item(), steps)\n",
    "\n",
    "            best_acc, best_loss = save_model(net, opt, loss, best_loss, acc, best_acc, steps, root, lr_scheduler=lr_scheduler, scaler=scaler)\n",
    "\n",
    "            print(\n",
    "                \"test: Epoch {} | Iters {} / {} | ms/batch {:5.2f} | acc/best acc/loss {:.2f} {:.2f} {:.2f} {:.2f}\".format(\n",
    "                    epoch,\n",
    "                    iterno,\n",
    "                    len(test_loader),\n",
    "                    1000 * (time.time() - start) / args.log_interval,\n",
    "                    acc,\n",
    "                    best_acc,\n",
    "                    loss,\n",
    "                    best_loss,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            print(\"Took %5.4fs to save samples\" % (time.time() - st))\n",
    "            print(\"-\" * 100)\n",
    "            net.train()\n",
    "\n",
    "    t_epoch = time.time() - t_epoch\n",
    "    print(\"epoch {}/{} time {:.2f}\".format(epoch, args.n_epochs, t_epoch / args.log_interval))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
